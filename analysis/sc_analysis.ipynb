{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from scipy.ndimage.interpolation import geometric_transform\n",
    "import statsmodels.api as sm\n",
    "from pathlib import Path\n",
    "import re\n",
    "import os\n",
    "from simcore_tools import FileRenamer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import (\n",
    "    KMeans, SpectralClustering, AffinityPropagation, AgglomerativeClustering)\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_param(string, param):\n",
    "    string = string.rpartition('/')[-1]\n",
    "    string = string.rpartition('.')[0]\n",
    "    start = string.find(param)\n",
    "    if start == -1:\n",
    "        if param == 'reload':\n",
    "            return '000'\n",
    "        else:\n",
    "            return 'NA'\n",
    "    end = string[start:].find('_')\n",
    "    if end == -1:\n",
    "        return string[start+len(param):]\n",
    "    else:\n",
    "        return string[start+len(param):start+end]\n",
    "\n",
    "\n",
    "def createFileList(extension, dirName='.'):\n",
    "    \"\"\"Returns a pandas series consisting of all files with the provided\n",
    "    extension in directory dirName, defaulting to the current working\n",
    "    directory.\n",
    "    \"\"\"\n",
    "    file_names = [os.path.join(dirName, file) for file in os.listdir(dirName) \n",
    "                  if file.rpartition('.')[-1] == extension]\n",
    "    if len(file_names) == 0:\n",
    "        raise ValueError(\"No '\" + extension + \"' files found in directory '\"\n",
    "              + dirName + \"'.\")\n",
    "    if re.search('v[0-9]{3}', file_names[0]):\n",
    "        raise ValueError(\"Filename found with 'vXXX' in name. \"\n",
    "                         \"Did you forget to rename the filenames before running the analysis?\")\n",
    "    file_names.sort()\n",
    "    return pd.Series(file_names)\n",
    "\n",
    "def check_directory_exists(dirname):\n",
    "    if not Path(dirname).is_dir():\n",
    "        print(\"Save directory not found:\", dirname)\n",
    "        var = input(\"Create it? (y/N) \")\n",
    "        if (var == 'y' or var == 'Y'):\n",
    "            try:\n",
    "                os.mkdir(dirname)\n",
    "            except Exception as e:\n",
    "                print(\"Creation of directory\", dirname, \"failed!\")\n",
    "                raise e\n",
    "        else:\n",
    "            raise ValueError(\"Save directory does not exist:\", dirname)\n",
    "\n",
    "def initializeDataFrame(dirName='.',\n",
    "                        params=['pf', 'sp', 'lp'],\n",
    "                        analyses=['global_order']):\n",
    "    \"\"\"Generates dataframe of all simcore analysis files, assuming\n",
    "    file naming convention of containing the substrings listed in the\n",
    "    params list, followed by the parameter quantity and an underscore.\n",
    "    This function tabulates these quantities using the file names of any\n",
    "    bitmaps (final state snapshots) found in snapshotDir. It then looks\n",
    "    for the corresponding analyses files whose extension is given by the\n",
    "    substrings found in the analyses list.\n",
    "    \"\"\"\n",
    "    df = None\n",
    "    params = params + ['reload']\n",
    "    for analysis in analyses:\n",
    "        analysis_df = pd.DataFrame(data=createFileList(analysis, dirName), columns=[analysis])\n",
    "        for param in params:\n",
    "            analysis_df[param] = analysis_df[analysis].apply(find_param, args=(param,))\n",
    "        if df is not None:\n",
    "            df = pd.merge(df, analysis_df, how='outer', on=params)\n",
    "        else:\n",
    "            df = analysis_df\n",
    "    return df[params + analyses]\n",
    "\n",
    "def check_dataframe(df):\n",
    "    for fname in df.global_order:\n",
    "        try:\n",
    "            pd.read_csv(fname, delim_whitespace=True, \n",
    "                        skiprows=1, index_col='time').dropna()\n",
    "        except Exception:\n",
    "            raise ValueError(\"Failed to read file {}\".format(fname))\n",
    "    for fname in df.polar_order_avg:\n",
    "        try:\n",
    "            pd.read_csv(fname, skiprows=1, delim_whitespace=True)\n",
    "        except Exception:\n",
    "            raise ValueError(\"Failed to read file {}\".format(fname))\n",
    "    for fname in df.flock:\n",
    "        try:\n",
    "            pd.read_csv(fname, header=3, low_memory=False, delim_whitespace=True)\n",
    "        except Exception:\n",
    "            raise ValueError(\"Failed to read file {}\".format(fname))\n",
    "    print(\"Everything checks out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_order_data(df, saveDirName=\".\", make_plots=True, params=['pf', 'sp', 'lp'], \n",
    "                          late_fraction=0.1, rolling_window=20):\n",
    "    \"\"\"Generates two grids of plots displaying time series of the global\n",
    "    order parameters, including the global polar/nematic order on one\n",
    "    figure and global spiral number/spiral handedness on a second figure.\n",
    "    \"\"\"\n",
    "    \n",
    "    analyze = 'global_order'\n",
    "    check_directory_exists(saveDirName)\n",
    "    gby = df.groupby(params)\n",
    "    row_list = []\n",
    "    for values, group in gby:\n",
    "        param_values = [i for pair in zip(params, values) for i in pair]\n",
    "        string_values = str.join('_', ['{}{}' for i in range(len(params))])\n",
    "        string_values = string_values.format(*param_values)\n",
    "        display_values = str.join(', ', ['{}={}' for i in range(len(params))])\n",
    "        display_values = display_values.format(*param_values)\n",
    "        \n",
    "        print(\"Gathering\", analyze, \"data for parameters\", display_values)\n",
    "        goDF = None\n",
    "        for file in group[analyze].sort_values():\n",
    "            if goDF is not None: \n",
    "                goDF = goDF.append(GetGlobalOrderDF(file), ignore_index=True)\n",
    "            else:\n",
    "                goDF = GetGlobalOrderDF(file)\n",
    "\n",
    "        if (make_plots):\n",
    "            plotDF = goDF.rolling(rolling_window).mean().dropna()\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "            plotDF.plot(y=\"nematic_order_mag\",color='blue',linewidth=1,ax=ax[0])\n",
    "            plotDF.plot(y=\"polar_order_mag\",color='red',linewidth=1,ax=ax[0])\n",
    "            ax[0].set_xlabel('Time')\n",
    "            ax[0].set_ylabel('Orientational order')\n",
    "            ax[0].legend(['Nematic order', 'Polar order'])\n",
    "            ax[0].set_ylim(0, 1)\n",
    "            plotDF.plot(y=\"spiral_order\",color='blue',linewidth=1,ax=ax[1])\n",
    "            plotDF.plot(y=\"signed_spiral_order\",color='red',linewidth=1,ax=ax[1])\n",
    "            ax[1].set_xlabel('Time')\n",
    "            ax[1].legend(['Spiral order', 'Spiral handedness'])\n",
    "            ax[1].set_ylabel('Spiral order')\n",
    "            fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "            fig.suptitle(\"Global order parameters: \" + display_values)\n",
    "            print(\"Saving\", analyze, \"plots for parameters\", display_values)\n",
    "            fig.savefig(Path(saveDirName, string_values + \"_global_order.png\"))\n",
    "            plt.close(fig)\n",
    "            \n",
    "        result_names = ['global_polar', 'global_polar_std', \n",
    "                        'global_nematic', 'global_nematic_std',\n",
    "                        'global_spiral', 'global_spiral_std']\n",
    "        late_time = int((1 - late_fraction)*goDF.shape[0])\n",
    "        results = (goDF['polar_order_mag'].iloc[late_time:].mean(), \n",
    "                   goDF['polar_order_mag'].iloc[late_time:].std(),\n",
    "                   goDF['nematic_order_mag'].iloc[late_time:].mean(), \n",
    "                   goDF['nematic_order_mag'].iloc[late_time:].std(),\n",
    "                   goDF['spiral_order'].iloc[late_time:].mean(), \n",
    "                   goDF['spiral_order'].iloc[late_time:].std())\n",
    "        row = {key:value \n",
    "               for key, value \n",
    "               in (list(zip(params, values)) + list(zip(result_names, results)))}\n",
    "        row_list.append(row)\n",
    "    return pd.DataFrame(row_list)\n",
    "\n",
    "\n",
    "def CalculateGlobalOrderMagnitudes(df):\n",
    "    \"\"\"Given a global order dataframe, calculates magnitude of polar\n",
    "    order vector and maximum eigenvalue of nematic order tensor.\n",
    "    \"\"\"\n",
    "    df['polar_order_mag'] = np.sqrt(df.polar_order_x**2 \n",
    "                                    + df.polar_order_y**2 \n",
    "                                    + df.polar_order_z**2)\n",
    "    df['nematic_order_mag'] = df.apply(lambda x: maxEig(\n",
    "        x['nematic_order_xx'],\n",
    "        x['nematic_order_xy'],\n",
    "        x['nematic_order_yx'],\n",
    "        x['nematic_order_yy']), axis=1)\n",
    "\n",
    "    \n",
    "def maxEig(xx,xy,yx,yy):\n",
    "    \"\"\"Returns the max eigenvalue of 2D matrix with elements xx, xy,\n",
    "    yx, yy.\n",
    "    \"\"\"\n",
    "    return max(np.linalg.eig(np.array([[xx,xy],[yx,yy]]))[0])\n",
    "\n",
    "    \n",
    "def GetGlobalOrderDF(fname):\n",
    "    \"\"\"Calculates time series of global orders parameters (polar order\n",
    "    vector magnitude and maximum eigenvalues of nematic order tensor Q)\n",
    "    from .global_order file with name 'fname' and returns global order\n",
    "    dataframe.\n",
    "    \"\"\"\n",
    "    assert isinstance(fname,str), \"'fname' must be a string!\"\n",
    "    df = pd.read_csv(fname,delim_whitespace=True,skiprows=1,\n",
    "                     index_col='time').dropna()\n",
    "    CalculateGlobalOrderMagnitudes(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires time step due to output file header not providing the data...\n",
    "def get_polar_order_avg_data(df, time_step, saveDirName=\".\", make_plots=True, \n",
    "                             params=['pf', 'sp', 'lp'], late_fraction=0.1, rolling_window=20):\n",
    "    \"\"\"Generates two grids of plots displaying time series of the global\n",
    "    order parameters, including the global polar/nematic order on one\n",
    "    figure and global spiral number/spiral handedness on a second figure.\n",
    "    \"\"\"\n",
    "    analyze = 'polar_order_avg'\n",
    "    check_directory_exists(saveDirName)\n",
    "    gby = df.groupby(params)\n",
    "    row_list = []\n",
    "    for values, group in gby:\n",
    "        param_values = [i for pair in zip(params, values) for i in pair]\n",
    "        string_values = str.join('_', ['{}{}' for i in range(len(params))])\n",
    "        string_values = string_values.format(*param_values)\n",
    "        display_values = str.join(', ', ['{}={}' for i in range(len(params))])\n",
    "        display_values = display_values.format(*param_values)\n",
    "        print(\"Gathering\", analyze, \"data for parameters\", display_values)\n",
    "\n",
    "        goDF = None\n",
    "        for file in group[analyze].sort_values():\n",
    "            if goDF is not None: \n",
    "                goDF = goDF.append(pd.read_csv(file, skiprows=1, delim_whitespace=True), \n",
    "                                   ignore_index=True)\n",
    "            else:\n",
    "                goDF = pd.read_csv(file, skiprows=1, delim_whitespace=True)\n",
    "        goDF['time'] = goDF.index * time_step\n",
    "        if make_plots:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "            plotDF = goDF.rolling(rolling_window).mean().dropna()\n",
    "            plotDF.plot(x=\"time\", y=\"avg_polar_order\", color='red',\n",
    "                      linewidth=1, ax=ax, label=r'$\\langle p_i \\rangle$')\n",
    "            plotDF.plot(x=\"time\", y=\"avg_contact_number\", color='blue',\n",
    "                      linewidth=1, ax=ax, label=r'$\\langle c_i \\rangle$')\n",
    "            ax.set_xlabel('Time')\n",
    "            ax.set_ylabel('Order parameter')\n",
    "            ax.set_title('Average local polar order: ' + display_values)\n",
    "            ax.legend(loc='best')\n",
    "            fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "            print(\"Saving\", analyze, \"plots for parameters\", display_values)\n",
    "            fig.savefig(Path(saveDirName, string_values + \"_local_polar_order_avg.png\"))\n",
    "            plt.close(fig)\n",
    "        result_names = ['avg_polar_order', 'avg_polar_order_std', \n",
    "                        'avg_contact_number', 'avg_contact_number_std']\n",
    "        late_time = 1 - late_fraction\n",
    "        results = (goDF['avg_polar_order'].iloc[int(late_time*goDF.shape[0]):].mean(), \n",
    "                   goDF['avg_polar_order'].iloc[int(late_time*goDF.shape[0]):].std(),\n",
    "                   goDF['avg_contact_number'].iloc[int(late_time*goDF.shape[0]):].mean(), \n",
    "                   goDF['avg_contact_number'].iloc[int(late_time*goDF.shape[0]):].std())\n",
    "        row = {key:value \n",
    "               for key, value \n",
    "               in (list(zip(params, values)) + list(zip(result_names, results)))}\n",
    "        row_list.append(row)\n",
    "    return pd.DataFrame(row_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flock_df(fname):\n",
    "    df = pd.read_csv(fname, header=3, low_memory=False, delim_whitespace=True)\n",
    "    header = pd.read_csv(fname, header=1, nrows=1, delim_whitespace=True)\n",
    "    filcols = [col for col in df.columns if col[:3] == 'fil']\n",
    "    flockcols = [col for col in df.columns if col[:3] != 'fil']\n",
    "\n",
    "    flock_global = df[flockcols].dropna()\n",
    "    flock_global = flock_global[['n_flocking', 'n_exterior', 'n_interior']]\n",
    "\n",
    "    flockstates = df[filcols].dropna().values\n",
    "    n_filaments = flockstates.shape[1]\n",
    "\n",
    "    flockstates[flockstates == 2] = 3\n",
    "    diffs = pd.DataFrame(np.diff(flockstates, axis=0))\n",
    "\n",
    "    freqs = ['f_not_ext', 'f_not_int', 'f_ext_int','f_ext_not', 'f_int_ext', 'f_int_not']\n",
    "    flock_state = ['n_not', 'n_ext', 'n_int']\n",
    "    change_state = [1, 3, 2, -1, -2, -3]\n",
    "    #change_state = [-3, -2, -1, 1, 2, 3]\n",
    "    df = pd.DataFrame(columns=freqs + flock_state)\n",
    "    df['n_not'] = n_filaments - flock_global['n_flocking']\n",
    "    df['n_ext'] = flock_global['n_exterior']\n",
    "    df['n_int'] = flock_global['n_interior']\n",
    "    for freq, state in zip(freqs, change_state):\n",
    "        if freq[2:5] == 'int':\n",
    "            denom = df['n_int']\n",
    "        elif freq[2:5] == 'ext':\n",
    "            denom = df['n_ext']\n",
    "        elif freq[2:5] == 'not':\n",
    "            denom = df['n_not']\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected frequency\")\n",
    "        df[freq] = diffs[diffs==state].count(axis=1) / denom\n",
    "    df = df.iloc[1:-1, :].fillna(0)\n",
    "    step = 0.5 * header['nspec'][0] * header['delta'][0]\n",
    "    df['time'] = df.index * step\n",
    "    for state in flock_state:\n",
    "        df[state] = df[state] / n_filaments\n",
    "    df['n_tot'] = 1 - df['n_not']\n",
    "    return df\n",
    "\n",
    "def plot_flock_state(df, display_string, save_string, rolling_window=20):\n",
    "    # Rolling time average with window = 20, 1 tau for nspec = 1000, delta = 0.0001\n",
    "    time = df['time']\n",
    "    plotDF = df.rolling(rolling_window).mean().dropna()\n",
    "    plotDF['time'] = time.iloc[:plotDF.shape[0]]\n",
    "    # Now plot them\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    ax[0].set_title('Fraction of flocking filaments')\n",
    "    ax[0].plot(plotDF['time'], plotDF['n_tot'], label='total')\n",
    "    ax[0].plot(plotDF['time'], plotDF['n_ext'], label='exterior')\n",
    "    ax[0].plot(plotDF['time'], plotDF['n_int'], label='interior')\n",
    "    ax[0].set_xlabel('Time')\n",
    "    ax[0].set_ylabel('Filament fraction')\n",
    "    ax[0].legend(loc='best')\n",
    "    freq_cols = [col for col in df.columns if col[:2] == 'f_']\n",
    "    plotDF[['time'] + freq_cols].plot(x='time', ax=ax[1], title='Normalized flock switching rates')\n",
    "    ax[1].set_ylabel('Frequency')\n",
    "    ax[1].set_xlabel('Time')\n",
    "    fig.suptitle(\"Flock dynamics: \"+display_string)\n",
    "    fig.savefig(save_string)\n",
    "    print(\"Saving plots for parameters\", display_string)\n",
    "    plt.close(fig)\n",
    "\n",
    "def get_flock_data(df, saveDirName=\".\", make_plots=True, params=['pf', 'sp', 'lp'],\n",
    "                   late_fraction=0.1, rolling_window=20):\n",
    "    \"\"\"TODO\"\"\"\n",
    "    analyze = 'flock'\n",
    "    check_directory_exists(saveDirName)\n",
    "    gby = df.groupby(params)\n",
    "    row_list = []\n",
    "    for values, group in gby:\n",
    "        param_values = [i for pair in zip(params, values) for i in pair]\n",
    "        string_values = str.join('_', ['{}{}' for i in range(len(params))])\n",
    "        string_values = string_values.format(*param_values)\n",
    "        display_values = str.join(', ', ['{}={}' for i in range(len(params))])\n",
    "        display_values = display_values.format(*param_values)\n",
    "        print(\"Gathering\", analyze, \"data for parameters\", display_values)\n",
    "\n",
    "        flock_df = None\n",
    "        for file in group[analyze].sort_values():\n",
    "            if flock_df is not None: \n",
    "                flock_df = flock_df.append(get_flock_df(file), ignore_index=True)\n",
    "            else:\n",
    "                flock_df = get_flock_df(file)\n",
    "        flock_df['time'] = flock_df.index * flock_df['time'].iloc[0]\n",
    "        \n",
    "        if make_plots:\n",
    "            plot_flock_state(flock_df, display_values,\n",
    "                             Path(saveDirName, string_values + \"_flock.png\"),\n",
    "                             rolling_window)\n",
    "            \n",
    "        flock_col_names = [col for col in flock_df.columns if col != 'time']\n",
    "        result_names_std = [name + '_std' for name in flock_col_names]\n",
    "        result_names = [label \n",
    "                        for label_tuple \n",
    "                        in zip(flock_col_names, result_names_std) \n",
    "                        for label in label_tuple]\n",
    "        late_time = int((1 - late_fraction)*flock_df.shape[0])\n",
    "        results = [result\n",
    "                   for result_tuple\n",
    "                   in zip([flock_df[name].iloc[late_time:].mean() for name in flock_col_names],\n",
    "                          [flock_df[name].iloc[late_time:].std() for name in flock_col_names]) \n",
    "                   for result in result_tuple]\n",
    "        row = {key:value \n",
    "               for key, value \n",
    "               in (list(zip(params, values)) + list(zip(result_names, results)))}\n",
    "        row_list.append(row)\n",
    "    return pd.DataFrame(row_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ['pf', 'sp', 'lp', 'dr']\n",
    "analyses = [\n",
    "    'global_order',\n",
    "    'polar_order_avg',\n",
    "    'flock',\n",
    "]\n",
    "data_dir = Path('data/order_params/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yamls = list(data_dir.glob(\"*_v*.yaml\"))\n",
    "yamls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_replace = \"pf0.05_dr3_v[0-9]{3}\"\n",
    "replacement = \"pf{}_sp{:03d}_lp{:04d}_dr{:02d}\"\n",
    "formatter_contents = ['filament:packing_fraction', 'soft_potential_mag',\n",
    "                      'filament:perlen_ratio', 'filament:driving_factor']\n",
    "renamer = FileRenamer(to_replace, replacement, formatter_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path(data_dir, 'old_params')\n",
    "p.mkdir(exist_ok=True)\n",
    "for file in yamls:\n",
    "    renamer.rename(file, confirm=True)\n",
    "    file.replace(Path(p, file.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = initializeDataFrame(data_dir, params, analyses)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmpdf = df[((df.lp.astype(int) < 40) & (df.reload == '000'))]\n",
    "# tmp_go_results = get_global_order_data(tmpdf, 'order_params/plots', make_plots=False, params=params,\n",
    "#                                   late_fraction=0.1, rolling_window=20)\n",
    "# tmp_poa_results = get_polar_order_avg_data(tmpdf, time_step=0.05, saveDirName='order_params/plots', make_plots=False)\n",
    "# tmp_flock_results = get_flock_data(tmpdf, 'order_params/plots', make_plots=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_to_drop = []\n",
    "# for ix in df[((df.reload=='000') & (df.lp.astype(int) < 20))].index:\n",
    "#     labels_to_drop.append(ix+1)\n",
    "#     labels_to_drop.append(ix+2)\n",
    "#     labels_to_drop.append(ix+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(df[(df.sp.astype(int) == 40) & ((df.lp.astype(int) == 10) | (df.lp.astype(int) == 20))].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(df[(df.sp.astype(int) == 200) & ((df.lp.astype(int) == 10) | (df.lp.astype(int) == 20))].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(df[(df.sp.astype(int) == 90) & (df.lp.astype(int) == 200)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(index=labels_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "go_results = get_global_order_data(df, Path(data_dir, 'plots'), make_plots=True, params=params,\n",
    "                                  late_fraction=0.1, rolling_window=20)\n",
    "pd.to_pickle(go_results, Path(data_dir, 'go_results.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poa_results = get_polar_order_avg_data(df, time_step=0.05, saveDirName=Path(data_dir, 'plots'),\n",
    "                                       make_plots=True, params=params)\n",
    "pd.to_pickle(poa_results, Path(data_dir, 'poa_results.pkl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flock_results = get_flock_data(df, Path(data_dir, 'plots'), make_plots=True, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(flock_results, Path(data_dir, 'flock_results.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = pd.merge(go_results, poa_results, how='inner', on=['pf', 'sp', 'lp', 'dr'])\n",
    "k2 = pd.merge(k1, flock_results, how='inner', on=['pf', 'sp', 'lp', 'dr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = k2[[col for col in k2.columns if col[-3:] != 'std']]\n",
    "results_std = k2[['pf', 'sp', 'lp', 'dr'] + [col for col in k2.columns if col[-3:] == 'std']]\n",
    "pd.to_pickle(results, Path(data_dir, 'results.pkl'))\n",
    "pd.to_pickle(results_std, Path(data_dir, 'results_std.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_pickle(Path(data_dir,'pkls/results_no_lowlp'))\n",
    "results_train = results[results.sp.astype(int) < 200]\n",
    "results_train = results_train[results_train.lp.astype(int) <= 100]\n",
    "#results_train = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 5\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(results_train.iloc[:, len(params):])\n",
    "X = scaler.fit_transform(results.iloc[:, len(params):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = scaler.fit_transform(results.iloc[:, len(params):])\n",
    "pca = PCA(n_components=5)\n",
    "pca_fit = pca.fit(X_train)\n",
    "X_train = np.matmul(X_train, pca.components_.transpose())\n",
    "X = np.matmul(X, pca.components_.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster(cluster_fit, tsne_fit, X, results, save_to_dir='.', color_bg=False,\n",
    "                 bg_fitter=KNeighborsClassifier(n_neighbors=3, metric='manhattan'),\n",
    "                 bg_interpolation='gaussian', color_order = None, forced_labels=None):\n",
    "    cluster_algo = str(cluster_fit.__class__).rpartition('.')[-1][:-2].lower()\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    if forced_labels is not None:\n",
    "        labels=forced_labels\n",
    "    else:\n",
    "        labels = cluster_fit.fit_predict(X)\n",
    "    light_colors = get_light_colors(labels, color_order)\n",
    "    dark_colors = get_dark_colors(labels, color_order)\n",
    "    predict_colors = [dark_colors[i] for i in labels]\n",
    "    sps = results['sp'].astype(int)\n",
    "    lps = results['lp'].astype(int)\n",
    "    if color_bg:\n",
    "        try:\n",
    "            assert bg_fitter.__module__.partition('.')[0] == 'sklearn'\n",
    "        except Exception:\n",
    "            raise ValueError(\"bg_fitter variable must point to initialized sklearn fitting function!\")\n",
    "        grid = []\n",
    "        for s in sps.unique():\n",
    "            for l in lps.unique():\n",
    "                grid.append([s,l])\n",
    "        grid = np.array(grid)\n",
    "        x_min, x_max = grid[:, 0].min() - 5, grid[:, 0].max() + 5\n",
    "        y_min, y_max = grid[:, 1].min() - 5, grid[:, 1].max() + 5\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, 1),\n",
    "                             np.arange(y_min, y_max, 1))\n",
    "        try:\n",
    "            grid_fit = bg_fitter.fit(grid, labels)\n",
    "        except:\n",
    "            raise ValueError(\"bg_fitter failed to fit bg color grid!\")\n",
    "        grid_fit_predict = grid_fit.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "        bg_colors = np.array([dark_colors[i] for i in grid_fit_predict]).reshape((xx.shape[0], xx.shape[1], 4))\n",
    "        ax[0].imshow(bg_colors, interpolation=bg_interpolation, aspect='auto', alpha=0.4, zorder=-100)\n",
    "        ax[0].invert_yaxis()\n",
    "    ax[0].scatter(sps, lps, c=[dark_colors[i] for i in labels])#, s=150)\n",
    "    ax[0].set_xlabel(\"$\\epsilon$ ($k_BT$)\")#, fontsize=30)\n",
    "    ax[0].set_ylabel(\"$L_p/L$\")#, fontsize=30)\n",
    "    ax[0].set_title(\"Clustering classification: \" + cluster_algo)#, fontsize=30)\n",
    "    #ax[0].tick_params(labelsize=20)\n",
    "\n",
    "    #sps = sorted(results['sp'].unique().astype(int))\n",
    "    #lps = sorted(results['lp'].unique().astype(int))\n",
    "    #ax[0].tick_params(which='major', width=1.00, length=6)\n",
    "    #ax[0].xaxis.set_major_formatter(FuncFormatter(lambda a, b: sps[b]))\n",
    "    #ax[0].yaxis.set_major_formatter(FuncFormatter(lambda a, b: lps[b]))\n",
    "    #ax[0].xaxis.set_major_locator(ticker.FixedLocator(6*np.array(sps)/max(sps)))\n",
    "    #ax[0].yaxis.set_major_locator(ticker.FixedLocator(6*np.array(lps)/max(lps)))\n",
    "\n",
    "    ax[1].scatter(tsne_fit.embedding_[:,0], tsne_fit.embedding_[:,1], c=[light_colors[i] for i in labels])\n",
    "    ax[1].set_xlabel(\"Embedded space, x\")\n",
    "    ax[1].set_ylabel(\"Embedded space, y\")\n",
    "    ax[1].set_title(\"TSNE Projection\")\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    fig.savefig(Path(save_to_dir, \"clustering_fit_\" + cluster_algo + \".png\"), dpi=600)\n",
    "    #ax[0].tick_params(labelsize=20)\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(sp, lp):\n",
    "    return results[(results.lp.astype(int) == lp) & (results.sp.astype(int) == sp)].index[0]\n",
    "\n",
    "def get_dark_colors(labels, order=None):\n",
    "#    cmap = plt.get_cmap('tab10')\n",
    "    alpha = 255\n",
    "    red = [139,0,0, alpha]\n",
    "    green = [0,100,0, alpha]\n",
    "    blue = [0,0,128,alpha]\n",
    "    teal = [0,128,128, alpha]\n",
    "    orange = [184,134,11, alpha]\n",
    "    if order is not None:\n",
    "        cmap = {'r': red,\n",
    "               'b': blue,\n",
    "               'g': green,\n",
    "               'c': teal,\n",
    "               'o': orange}\n",
    "        colors = [cmap[c] for c in order]\n",
    "    else:\n",
    "        colors = [red, green, blue, teal, orange]\n",
    "    return [[rgb/255 for rgb in color] for color in colors]\n",
    "\n",
    "def get_light_colors(labels, order=None):\n",
    "    alpha = 255\n",
    "    red = [205,0,0, alpha]\n",
    "    green = [0,205,0, alpha]\n",
    "    blue = [0,0,205,alpha]\n",
    "    teal = [0,150,150, alpha]\n",
    "    orange = [218,165,32, alpha]\n",
    "    if order is not None:\n",
    "        cmap = {'r': red,\n",
    "               'b': blue,\n",
    "               'g': green,\n",
    "               'c': teal,\n",
    "               'o': orange}\n",
    "        colors = [cmap[c] for c in order]\n",
    "    else:\n",
    "        colors = [red, green, blue, teal, orange]\n",
    "    return [[rgb/255 for rgb in color] for color in colors]\n",
    "\n",
    "#    colors = cmap(np.linspace(0, 1, len(np.unique(labels))))\n",
    "    #return [colors[i] for i in labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.random.randint(100, 10000)\n",
    "#state = 1158\n",
    "tsne = TSNE(perplexity=14, random_state=state, metric='euclidean').fit(X_train)\n",
    "plt.figure()\n",
    "plt.scatter(tsne.embedding_[:,0], tsne.embedding_[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1, weights='uniform', p=1, )\n",
    "svc = SVC(kernel=30*RBF(25), gamma='auto', tol=1e-6,\n",
    "          probability=True, max_iter=1e8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.fit_predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[-36] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[-11-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[-55+2] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state = np.random.raandint(100, 10000)\n",
    "# default color_order = ['r', 'g', 'b', 'c', 'o']\n",
    "kmeans = KMeans(n_clusters=5, init='k-means++', n_init=20, max_iter=500,\n",
    "                random_state=state).fit(X)\n",
    "plot_cluster(kmeans, tsne, X_train, results_train,\n",
    "             Path(data_dir, 'plots'), True, bg_fitter=svc, bg_interpolation='gaussian',\n",
    "             color_order = ['b','g','r','c','o'], forced_labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral = SpectralClustering(n_clusters=4, gamma=0.01).fit(X)\n",
    "plot_cluster(spectral, tsne, X_train, results_train, Path(data_dir, 'plots'), True, \n",
    "             bg_fitter=svc, bg_interpolation='gaussian',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglom = AgglomerativeClustering(n_clusters=4).fit(X)\n",
    "plot_cluster(agglom, tsne, X_train, results_train, Path(data_dir, 'plots'), True,\n",
    "             bg_fitter=svc, bg_interpolation='gaussian',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmix = GaussianMixture(n_components=4, max_iter=1000, n_init=10).fit(X)\n",
    "plot_cluster(gmix, tsne, X_train, results_train, Path(data_dir, 'plots'), True,\n",
    "            bg_fitter=svc, bg_interpolation='gaussian',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('./tmp_plots').mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = []\n",
    "for nc in range(1, 10):\n",
    "    km_tmp = KMeans(n_clusters=nc, init='k-means++', n_init=20, max_iter=500, random_state=state).fit(X)\n",
    "    inertias.append(km_tmp.inertia_)\n",
    "    plot_cluster(km_tmp, tsne, X, Path('./tmp_plots'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(inertias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xdata = X.copy()\n",
    "\n",
    "# Labeling the training data for supervised learning\n",
    "#y=[0,0,0, 0,0,1, 1,1,2, 1,2,2, 3,4,4, 3,4,4, 2,2] \n",
    "y = kmeans.fit_predict(X)\n",
    "clf = KNeighborsClassifier(n_neighbors=5, metric='manhattan').fit(xx, y)\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = np.array([colors[i] for i in Z])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape((xx.shape[0], xx.shape[1], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = []\n",
    "sps = results.sp.astype(int).unique()\n",
    "lps = results.lp.astype(int).unique()\n",
    "# Build the dataset of training points\n",
    "for s in sps:\n",
    "    for l in lps:\n",
    "        xx.append([s,l])\n",
    "        \n",
    "#X.append([30,100])\n",
    "#X.append([60,100])\n",
    "\n",
    "xx = np.array(xx)\n",
    "\n",
    "# Fit the data using a gaussian kernel\n",
    "# The variance and magnitude of the kernel was found by\n",
    "# trial and error to generate sensible phase boundaries.\n",
    "kernel = 50*RBF(20)\n",
    "#clf = SVC(kernel=kernel, gamma='auto', tol=1e-6,\n",
    "#          probability=True, max_iter=1e8).fit(xx, y)\n",
    "clf = KNeighborsClassifier(n_neighbors=5, metric='manhattan').fit(xx, y)\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = np.array([colors[i] for i in Z])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape((xx.shape[0], xx.shape[1], 4))\n",
    "# create a mesh for the colorplot\n",
    "x_min, x_max = xx[:, 0].min() - 10, xx[:, 0].max() + 10\n",
    "y_min, y_max = xx[:, 1].min() - 10, xx[:, 1].max() + 10\n",
    "\n",
    "h = .01  # step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "alpha = 0.4\n",
    "red = [1,0,0,alpha]\n",
    "lime = [0,1,0,alpha]\n",
    "blue = [0,0,1,alpha]\n",
    "cyan = [0,1,1,alpha]\n",
    "magenta = [1,0,1,alpha]\n",
    "\n",
    "red_patch = mpatches.Patch(color=red[:3],\n",
    "                           label='Active isotropic')\n",
    "lime_patch = mpatches.Patch(color=lime[:3],\n",
    "                            label='Flocking')\n",
    "blue_patch = mpatches.Patch(color=blue[:3],\n",
    "                            label='Polar band')\n",
    "cyan_patch = mpatches.Patch(color=cyan[:3],\n",
    "                            label='Spooling')\n",
    "magenta_patch = mpatches.Patch(color=magenta[:3],\n",
    "                               label='Turbulent')\n",
    "colors = np.array([red, lime, blue, cyan, magenta])\n",
    "\n",
    "#plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Plot the predicted probabilities. For that, we will \n",
    "# assign a color to each point in the mesh\n",
    "# [x_min, m_max]x[y_min, y_max].\n",
    "\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = np.array([colors[i] for i in Z])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape((xx.shape[0], xx.shape[1], 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "#ax = fig.gca(projection='3d')\n",
    "\n",
    "plt.imshow(Z, origin=\"lower\")\n",
    "\n",
    "# Plot the training points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "#ax = fig.gca(projection='3d')\n",
    "\n",
    "plt.imshow(Z, origin=\"lower\",extent=(x_min, x_max, y_min, y_max))\n",
    "\n",
    "# Plot the training points\n",
    "#plt.scatter(xx[:, 0], xx[:, 1])#, #s=80,\n",
    "            #c=[list(colors[i][:3]) for i in y], \n",
    "            #edgecolors=(0, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sps = results.sp.astype(int).unique()\n",
    "lps = results.lp.astype(int).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sps.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.predict(X)\n",
    "\n",
    "\n",
    "\n",
    "#bg_colors = np.rot90(\n",
    "#   np.array(predict_colors).reshape((results['sp'].nunique(), results['lp'].nunique(), 4))\n",
    "#)[::-1]\n",
    "ax[0].imshow(bg_colors, interpolation='gaussian', aspect='auto', alpha=0.7, zorder=-100)\n",
    "ax[0].invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sps.shape[0] * lps.shape[0] * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_dummy(sps, lps, labels, save_to_dir='.', color_bg=False,\n",
    "                 bg_fitter=KNeighborsClassifier(n_neighbors=3, metric='manhattan'),\n",
    "                 bg_interpolation='gaussian', color_order = None, save_name='dummy'):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    light_colors = get_light_colors(labels, color_order)\n",
    "    dark_colors = get_dark_colors(labels, color_order)\n",
    "    predict_colors = [dark_colors[i] for i in labels]\n",
    "    if color_bg:\n",
    "        try:\n",
    "            assert bg_fitter.__module__.partition('.')[0] == 'sklearn'\n",
    "        except Exception:\n",
    "            raise ValueError(\"bg_fitter variable must point to initialized sklearn fitting function!\")\n",
    "        grid = []\n",
    "        for s in np.unique(sps):\n",
    "            for l in np.unique(lps):\n",
    "                grid.append([s,l])\n",
    "        grid = np.array(grid)\n",
    "        x_min, x_max = 0, grid[:, 0].max() + 5\n",
    "        y_min, y_max = 0, grid[:, 1].max() + 5\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, 1),\n",
    "                             np.arange(y_min, y_max, 1))\n",
    "        try:\n",
    "            grid_fit = bg_fitter.fit(grid, labels)\n",
    "        except:\n",
    "            raise ValueError(\"bg_fitter failed to fit bg color grid!\")\n",
    "        grid_fit_predict = grid_fit.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "        bg_colors = np.array([dark_colors[i] for i in grid_fit_predict]).reshape((xx.shape[0], xx.shape[1], 4))\n",
    "        ax[0].imshow(bg_colors, interpolation=bg_interpolation, aspect='auto', alpha=0.4, zorder=-100)\n",
    "        ax[0].invert_yaxis()\n",
    "    ax[0].scatter(sps, lps, c=[dark_colors[i] for i in labels])#, s=150)\n",
    "    ax[0].set_xlabel(\"$\\epsilon$ ($k_BT$)\")#, fontsize=30)\n",
    "    ax[0].set_ylabel(\"$L_p/L$\")#, fontsize=30)\n",
    "    ax[0].set_title(\"Clustering classification: kmeans\")#, fontsize=30)\n",
    "    #ax[1].scatter(tsne_fit.embedding_[:,0], tsne_fit.embedding_[:,1])\n",
    "    ax[1].scatter(tsne.embedding_[:,0], tsne.embedding_[:,1])#c=[light_colors[i] for i in labels])\n",
    "    ax[1].set_xlabel(\"Embedded space, x\")\n",
    "    ax[1].set_ylabel(\"Embedded space, y\")\n",
    "    ax[1].set_title(\"TSNE Projection\")\n",
    "\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    fig.savefig(Path(save_to_dir, save_name + \"_clustering_fit_kmeans.png\"), dpi=600)\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sps = np.array([15, 15, 15, 25, 25, 25, 50, 50, 50, 100, 100, 100])\n",
    "lps = np.array([20, 50, 100, 20, 50, 100, 20, 50, 100, 20, 50, 100])\n",
    "labels=np.array([1, 3, 3, 2, 3, 3, 2, 3, 3, 4, 5, 5])-1\n",
    "svc = SVC(kernel=50*RBF(35), gamma='auto', tol=1e-6,\n",
    "          probability=True, max_iter=1e8)\n",
    "plot_cluster_dummy(sps, lps, labels, '.', True, bg_fitter=svc, bg_interpolation='gaussian',\n",
    "             color_order = ['g','b','c','o','r'], save_name='pf0.4_dr30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sps = np.array([15, 15, 15, 20, 20, 20, 25, 25, 25, 50, 50, 50, 80, 80, 80, 100, 100, 100])\n",
    "lps = np.array([20, 50, 100, 20, 50, 100, 20, 50, 100, 20, 50, 100, 20, 50, 100, 20, 50, 100])\n",
    "labels=np.array([1, 1, 1, 1, 1, 2, 2, 2, 3, 2, 3, 3, 4, 5, 5, 4, 5, 5])-1\n",
    "svc = SVC(kernel=30*RBF(20), gamma='auto', tol=1e-6,\n",
    "          probability=True, max_iter=1e8)\n",
    "plot_cluster_dummy(sps, lps, labels, '.', True, bg_fitter=svc, bg_interpolation='gaussian',\n",
    "             color_order = ['g','b','c','o','r'], save_name='pf0.2_dr30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sps = np.array([15, 15, 15, 25, 25, 25, 50, 50, 50, 100, 100, 100])\n",
    "lps = np.array([20, 50, 100, 20, 50, 100, 20, 50, 100, 20, 50, 100])\n",
    "labels=np.array([1, 1, 1, 1, 2, 2, 2, 2, 3, 4, 4, 2])\n",
    "plot_cluster_dummy(sps, lps, labels, '.', True, bg_fitter=svc, bg_interpolation='gaussian',\n",
    "             color_order = ['r','g','b','c','o'], save_name='pf0.20_dr30')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
